{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Image project\n",
    "\n",
    "**Due Thursday, December 13, before 23:59.**\n",
    "\n",
    "The task is to learn to assign the correct labels to a set of images.  The images are originally from a photo-sharing site and released under Creative Commons-licenses allowing sharing.  The training set contains 20 000 images. We have resized them and cropped them to 128x128 to make the task a bit more manageable.\n",
    "\n",
    "We're only giving you the code for downloading the data. The rest you'll have to do yourselves.\n",
    "\n",
    "Some comments and hints particular to the image project:\n",
    "\n",
    "- One image may belong to many classes in this problem, i.e., it's a multi-label classification problem. In fact there are images that don't belong to any of our classes, and you should also be able to handle these correctly. Pay careful attention to how you design the outputs of the network (e.g., what activation to use) and what loss function should be used.\n",
    "\n",
    "- As the dataset is pretty imbalanced, don't focus too strictly on the outputs being probabilistic. (Meaning that the right threshold for selecting the label might not be 0.5.)\n",
    "\n",
    "- Image files can be loaded as numpy matrices for example using `imread` from `matplotlib.pyplot`. Most images are color, but a few grayscale. You need to handle the grayscale ones somehow as they would have a different number of color channels (depth) than the color ones.\n",
    "\n",
    "- In the exercises we used e.g., `torchvision.datasets.MNIST` to handle the loading of the data in suitable batches. Here, you need to handle the dataloading yourself.  The easiest way is probably to create a custom `Dataset`. [See for example here for a tutorial](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets.utils import download_url\n",
    "import zipfile\n",
    "\n",
    "train_path = 'train'\n",
    "dl_file = 'dl2018-image-proj.zip'\n",
    "dl_url = 'https://users.aalto.fi/mvsjober/misc/'\n",
    "\n",
    "zip_path = os.path.join(train_path, dl_file)\n",
    "if not os.path.isfile(zip_path):\n",
    "    download_url(dl_url + dl_file, root=train_path, filename=dl_file, md5=None)\n",
    "\n",
    "with zipfile.ZipFile(zip_path) as zip_f:\n",
    "    zip_f.extractall(train_path)\n",
    "    #os.unlink(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command downloaded and extracted the data files into the `train` subdirectory.\n",
    "\n",
    "The images can be found in `train/images`, and are named as `im1.jpg`, `im2.jpg` and so on until `im20000.jpg`.\n",
    "\n",
    "The class labels, or annotations, can be found in `train/annotations` as `CLASSNAME.txt`, where CLASSNAME is one of the fourteen classes: *baby, bird, car, clouds, dog, female, flower, male, night, people, portrait, river, sea,* and *tree*.\n",
    "\n",
    "Each annotation file is a simple text file that lists the images that depict that class, one per line. The images are listed with their number, not the full filename. For example `5969` refers to the image `im5969.jpg`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your stuff goes here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and preparing Dataloader\n",
    "\n",
    "First we format the labels to usable form. Saved as csv to `./train/labels.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         img  baby  bird  car  clouds  dog  female  flower  male  night  \\\n",
      "0          1     0     0    0       0    0       1       0     0      0   \n",
      "1          2     0     0    0       0    0       0       0     0      0   \n",
      "2          3     0     0    0       0    0       0       0     0      0   \n",
      "3          4     0     0    0       0    0       0       0     1      0   \n",
      "4          5     0     0    0       0    0       0       0     1      0   \n",
      "5          6     0     0    0       0    0       0       0     0      0   \n",
      "6          7     0     0    0       0    0       1       0     0      0   \n",
      "7          8     0     0    0       0    0       0       0     0      0   \n",
      "8          9     0     0    0       0    0       0       0     0      0   \n",
      "9         10     0     0    0       0    0       0       0     0      0   \n",
      "10        11     0     0    0       0    0       0       0     1      0   \n",
      "11        12     0     0    0       0    0       1       0     0      0   \n",
      "12        13     0     0    0       0    0       0       0     1      0   \n",
      "13        14     0     0    0       0    0       0       0     0      0   \n",
      "14        15     0     0    0       1    0       0       0     0      0   \n",
      "15        16     0     0    0       0    0       0       0     0      1   \n",
      "16        17     0     0    0       0    0       0       0     0      0   \n",
      "17        18     0     0    0       0    0       0       0     0      0   \n",
      "18        19     0     0    0       0    0       0       0     0      0   \n",
      "19        20     0     0    0       0    0       0       0     0      0   \n",
      "20        21     0     0    0       1    0       0       0     0      0   \n",
      "21        22     0     0    0       0    0       0       0     0      0   \n",
      "22        23     0     0    0       0    0       0       0     0      0   \n",
      "23        24     0     0    0       0    0       0       0     1      0   \n",
      "24        25     0     0    0       0    0       0       0     0      0   \n",
      "25        26     0     0    0       0    0       1       0     0      0   \n",
      "26        27     0     0    0       0    0       0       0     0      0   \n",
      "27        28     0     0    0       0    0       0       1     0      0   \n",
      "28        29     0     0    0       0    0       0       0     0      0   \n",
      "29        30     0     0    0       0    0       0       0     0      0   \n",
      "...      ...   ...   ...  ...     ...  ...     ...     ...   ...    ...   \n",
      "19970  19971     0     0    0       0    0       1       0     0      0   \n",
      "19971  19972     0     0    0       0    0       0       0     0      0   \n",
      "19972  19973     0     0    0       1    0       0       0     0      0   \n",
      "19973  19974     0     0    0       0    0       0       0     1      0   \n",
      "19974  19975     0     0    0       0    0       0       0     0      0   \n",
      "19975  19976     0     0    0       0    0       0       0     0      0   \n",
      "19976  19977     0     0    0       0    0       0       0     0      0   \n",
      "19977  19978     0     0    0       0    0       0       0     0      0   \n",
      "19978  19979     0     0    0       0    0       0       0     0      0   \n",
      "19979  19980     0     0    0       0    0       0       0     0      0   \n",
      "19980  19981     0     0    0       0    0       0       0     1      0   \n",
      "19981  19982     0     0    0       0    0       0       0     0      0   \n",
      "19982  19983     0     0    0       0    0       0       0     0      0   \n",
      "19983  19984     0     0    0       0    0       1       0     1      0   \n",
      "19984  19985     0     0    0       0    0       0       0     0      0   \n",
      "19985  19986     0     0    0       0    0       1       0     0      0   \n",
      "19986  19987     0     0    0       0    0       0       0     0      0   \n",
      "19987  19988     0     0    0       0    0       0       0     0      0   \n",
      "19988  19989     0     0    0       0    0       1       0     0      0   \n",
      "19989  19990     0     0    0       0    0       0       0     0      0   \n",
      "19990  19991     0     0    0       0    0       0       1     0      0   \n",
      "19991  19992     0     0    0       0    0       0       0     1      0   \n",
      "19992  19993     0     0    0       0    0       0       0     0      0   \n",
      "19993  19994     0     0    0       0    0       0       0     0      0   \n",
      "19994  19995     0     0    0       0    0       0       0     0      0   \n",
      "19995  19996     0     0    0       1    0       0       0     0      0   \n",
      "19996  19997     0     0    0       0    0       0       0     1      0   \n",
      "19997  19998     0     0    0       0    0       0       0     0      0   \n",
      "19998  19999     0     0    0       0    0       1       0     0      0   \n",
      "19999  20000     0     0    0       0    0       0       0     1      0   \n",
      "\n",
      "       people  portrait  river  sea  tree  \n",
      "0           1         1      0    0     0  \n",
      "1           0         0      0    0     0  \n",
      "2           0         0      0    0     0  \n",
      "3           1         0      0    0     0  \n",
      "4           1         0      0    0     0  \n",
      "5           0         0      0    0     0  \n",
      "6           1         0      0    0     0  \n",
      "7           0         0      0    0     0  \n",
      "8           0         0      0    0     0  \n",
      "9           0         0      0    0     0  \n",
      "10          1         0      0    0     0  \n",
      "11          1         1      0    0     0  \n",
      "12          0         0      0    0     0  \n",
      "13          0         0      0    0     0  \n",
      "14          0         0      0    0     1  \n",
      "15          1         0      0    0     0  \n",
      "16          0         0      0    0     0  \n",
      "17          0         0      0    0     0  \n",
      "18          0         0      0    0     0  \n",
      "19          0         0      0    0     0  \n",
      "20          0         0      0    0     0  \n",
      "21          0         0      0    0     0  \n",
      "22          0         0      0    0     0  \n",
      "23          1         0      0    0     0  \n",
      "24          1         0      0    0     0  \n",
      "25          1         1      0    0     0  \n",
      "26          0         0      0    0     0  \n",
      "27          0         0      0    0     0  \n",
      "28          0         0      0    0     0  \n",
      "29          0         0      0    0     0  \n",
      "...       ...       ...    ...  ...   ...  \n",
      "19970       1         1      0    0     0  \n",
      "19971       0         0      0    0     0  \n",
      "19972       0         0      0    0     0  \n",
      "19973       1         0      0    0     0  \n",
      "19974       0         0      0    0     0  \n",
      "19975       0         0      0    0     0  \n",
      "19976       0         0      0    0     0  \n",
      "19977       0         0      0    0     0  \n",
      "19978       0         0      0    0     0  \n",
      "19979       0         0      0    0     0  \n",
      "19980       1         1      0    0     0  \n",
      "19981       0         0      0    0     0  \n",
      "19982       0         0      0    0     0  \n",
      "19983       1         1      0    0     0  \n",
      "19984       0         0      0    0     0  \n",
      "19985       1         0      0    0     0  \n",
      "19986       0         0      0    0     0  \n",
      "19987       1         0      0    0     0  \n",
      "19988       1         1      0    0     0  \n",
      "19989       0         0      0    0     0  \n",
      "19990       0         0      0    0     0  \n",
      "19991       1         1      0    0     0  \n",
      "19992       0         0      0    0     0  \n",
      "19993       0         0      0    0     0  \n",
      "19994       0         0      0    0     0  \n",
      "19995       0         0      0    0     0  \n",
      "19996       1         0      0    0     0  \n",
      "19997       0         0      0    0     0  \n",
      "19998       1         1      0    0     0  \n",
      "19999       1         0      0    0     0  \n",
      "\n",
      "[20000 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import platform\n",
    "# Create an array with tuple(img, label) pairs from the annotations txt files.\n",
    "files = glob.glob(\"./train/annotations/*\")\n",
    "labels = []\n",
    "for name in files:\n",
    "    try:\n",
    "        with open(name) as f:\n",
    "            if platform.system == \"Windows\":\n",
    "                label = name.split(\"\\\\\")[1].split(\".\")[0]\n",
    "            else:\n",
    "                label = name.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "            for line in f:\n",
    "                labels.append((int(line.splitlines()[0]), label))\n",
    "\n",
    "    except IOError as exc: #Not sure what error this is\n",
    "        if exc.errno != errno.EISDIR:\n",
    "            raise\n",
    "            \n",
    "# Create dataframe with columns img, label and sort it by img and reset index without preserving old index values.\n",
    "labels = pd.DataFrame(labels, columns=[\"img\", \"label\"])\n",
    "# One hot encoding\n",
    "one_hot = pd.get_dummies(labels[\"label\"])\n",
    "labels = labels.join(one_hot).drop(\"label\", axis=\"columns\")\n",
    "labels = labels.groupby(\"img\", as_index=False).sum()\n",
    "labels[\"img\"] = labels[\"img\"].astype(\"int32\")\n",
    "imgs_present = labels[\"img\"].unique()\n",
    "rows_to_add = []\n",
    "for i in range(20000 - 1): # this takes a while sry\n",
    "    if i+1 in imgs_present:\n",
    "        continue\n",
    "    else:\n",
    "        rows_to_add.append([i+1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # kinda ugly lol\n",
    "df_to_append = pd.DataFrame(rows_to_add, columns=[\"img\",\"baby\", \"bird\",\n",
    "                                                  \"car\", \"clouds\", \"dog\",\n",
    "                                                  \"female\", \"flower\",\n",
    "                                                  \"male\",\"night\", \"people\",\n",
    "                                                  \"portrait\", \"river\", \"sea\",\n",
    "                                                  \"tree\"])\n",
    "\n",
    "labels = labels.append(df_to_append).sort_values(\"img\").reset_index(drop=True)\n",
    "print(labels)\n",
    "labels.to_csv(\"./train/labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10176 rows with any labels and 9824 images without a label.\n",
    "\n",
    "## Dataset and Dataloader\n",
    "We create our Dataset to be used through Dataloader to train and evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import imageio\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Images(Dataset):\n",
    "    def __init__(self): \n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        Here we can define transformations for the dataset aswell.\n",
    "        \"\"\"        \n",
    "        self.labels = pd.read_csv(\"./train/labels.csv\")\n",
    "        self.center_crop = transforms.CenterCrop(100)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        \n",
    "        self.transformations = transforms.Compose([transforms.CenterCrop(100), transforms.ToTensor()])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        index = index.item() + 1 #indexing starts from 1 in our dataset\n",
    "        try:\n",
    "            img = np.array(imageio.imread('./train/images/im' + str(index) + '.jpg', pilmode=\"RGB\"))\n",
    "        except:\n",
    "            raise\n",
    "        \n",
    "        img = self.to_tensor(img)\n",
    "       # e.g. data = self.center_crop(data)\n",
    "        label = labels[labels[\"img\"] == index].drop(\"img\", axis=\"columns\")\n",
    "        label = torch.from_numpy(np.array(label))\n",
    "        return (img, label.squeeze())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "batch_size = 1\n",
    "imgs = Images()\n",
    "\n",
    "train_size = int(0.8 * len(imgs))\n",
    "test_size = len(imgs) - train_size\n",
    "\n",
    "train_dataset, validation_dataset = torch.utils.data.random_split(imgs, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now use our `train_loader` as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 img torch.Size([1, 3, 128, 128]) label torch.Size([1, 14])\n",
      "batch 1 img torch.Size([1, 3, 128, 128]) label torch.Size([1, 14])\n",
      "batch 2 img torch.Size([1, 3, 128, 128]) label torch.Size([1, 14])\n",
      "batch 3 img torch.Size([1, 3, 128, 128]) label torch.Size([1, 14])\n",
      "batch 4 img torch.Size([1, 3, 128, 128]) label torch.Size([1, 14])\n",
      "and so on...\n"
     ]
    }
   ],
   "source": [
    "for idx, (X, y) in enumerate(train_loader):\n",
    "    if idx == 5:\n",
    "        print(\"and so on...\")\n",
    "        break\n",
    "    print (\"batch\", idx, \"img\", X.shape,\"label\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_to_labels(y):\n",
    "    \"\"\"\n",
    "    a little helper function to get labels from one-hot-encoded vectors\n",
    "    \"\"\"\n",
    "    labels = pd.read_csv(\"./train/labels.csv\").drop([\"Unnamed: 0\",\"img\"], axis=\"columns\").columns\n",
    "    return list(labels.take(np.where(np.array(y))).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lets see few examples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f, axarr = plt.subplots(3,3)\n",
    "f.subplots_adjust(hspace=0.9)\n",
    "\n",
    "for i, ax in enumerate(f.axes):\n",
    "    X, y = imgs.__getitem__(torch.tensor(i+999))\n",
    "    ax.axis('off')\n",
    "    ax.imshow(np.array(X.permute(1,2,0)))\n",
    "    ax.set_title(str(y_to_labels(y)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU.\n",
      "Net(\n",
      "  (fc): Linear(in_features=49152, out_features=49152, bias=True)\n",
      "  (conv1): Conv2d(3, 18, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (out): Linear(in_features=71442, out_features=14, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext import datasets, vocab\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Using gpu.')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('Using CPU.')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, output_size = 14, batch_size=16):\n",
    "        super(Net, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        self.fc = nn.Linear(128*128*3, 128*128*3)\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels=18, kernel_size=3)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.out = nn.Linear(18*63*63, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x = [batch_size, color_dim, x, y]\n",
    "        # flatten for first fully connected\n",
    "        flat = x.view(-1, 128*128*3)\n",
    "        h = F.relu(self.fc(flat))\n",
    "        # Convolutional layer expects the input to be of shape (batch_size, channel_dim, x_dim, y_dim)\n",
    "        h = h.view(-1, 3, 128,128)\n",
    "        conv = F.relu(self.conv1(h))\n",
    "        pooled = self.pool(conv)\n",
    "        # pooled.shape = (batch, 18, 63, 63)\n",
    "        pooled = pooled.view(-1, 18*63*63)\n",
    "        output = self.out(pooled)\n",
    "        return output\n",
    "\n",
    "model = Net(batch_size = batch_size).to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# I think Binary Cross Entropy is OK for multilabel. LogitsLoss is just added sigmoid with log-sum-exp trick\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for idx, (X, y) in enumerate(loader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(X)\n",
    "        \n",
    "        loss = criterion(predictions, y.float())\n",
    "                \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_f1 = 0\n",
    "    \n",
    "    model.eval() # disables normalizations like dropout, batchnorm etc..\n",
    "    \n",
    "    with torch.no_grad(): # disables autograd engine\n",
    "    \n",
    "         for idx, (X, y) in enumerate(loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            predictions = model(X)\n",
    "            \n",
    "            loss = criterion(predictions, y.float())\n",
    "            # just round for label true/false\n",
    "            rounded_preds = torch.round(torch.sigmoid(predictions)) # also sigmoid because its not used without loss function\n",
    "            \n",
    "            # micro-averaged f1 is used for determining the best model.\n",
    "            f1 = f1_score(rounded_preds, y, average='micro')\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_f1 += f1.item()\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_f1 / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started training epoch 1..\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 9GB. Buy new RAM! at /pytorch/aten/src/TH/THGeneral.cpp:204",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-24b4f11209c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"started training epoch {epoch + 1}..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"started validating epoch {epoch + 1}..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-bea11c2618fd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 9GB. Buy new RAM! at /pytorch/aten/src/TH/THGeneral.cpp:204"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"started training epoch {epoch + 1}..\")\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    print(f\"started validating epoch {epoch + 1}..\")\n",
    "    valid_loss, valid_f1 = evaluate(model, valid_loader, criterion)\n",
    "    print(f'Training epoch {epoch +1},  loss: {train_loss}')\n",
    "    print(f'Validation epoch {epoch +1}: loss: {valid_loss}, F1 score: {valid_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "It might be useful to save your model if you want to continue your work later, or use it for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file should now be visible in the \"Home\" screen of the jupyter notebooks interface.  There you should be able to select it and press \"download\".  [See more here on how to load the model back](https://github.com/pytorch/pytorch/blob/761d6799beb3afa03657a71776412a2171ee7533/docs/source/notes/serialization.rst) if you want to continue training later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download test set\n",
    "\n",
    "The testset will be made available during the last week before the deadline and can be downloaded in the same way as the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for test set\n",
    "\n",
    "You should return your predictions for the test set in a plain text file.  The text file contains one row for each test set image.  Each row contains a binary prediction for each label (separated by a single space), 1 if it's present in the image, and 0 if not. The order of the labels is as follows (alphabetic order of the label names):\n",
    "\n",
    "    baby bird car clouds dog female flower male night people portrait river sea tree\n",
    "\n",
    "An example row could like like this if your system predicts the presense of a bird and clouds:\n",
    "\n",
    "    0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
    "    \n",
    "The order of the rows should be according to the numeric order of the image numbers.  In the test set, this means that the first row refers to image `im20001.jpg`, the second to `im20002.jpg`, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have the prediction output matrix prepared in `y` you can use the following function to save it to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('results.txt', y, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
